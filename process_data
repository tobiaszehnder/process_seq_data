#! /bin/bash

### Script to process ChIP-seq data from fastq to bigwig

set -eE # exit when any command fails (e), and allow error trapping (E)
shopt -s extglob # extended globbing. e.g. for expanding wildcards in variables

# ---------------------------------------------------
# Functions
# ---------------------------------------------------

# cleanup trap
trap 'cleanup ${current_file}' ERR INT # if the script fails or is aborted by the user (CTRL+C), remove the file that is currently being written.
cleanup() { if [[ $1 != "NA" ]]; then echo -e "\nRemoving $1"; rm -f $1; exit 0; fi }

# script usage
usage() {
cat << EOF
Usage: prun mdl ${0##*/} -d DATA_DIR -t DATA_TABLE -n NTHREADS [optional: -o]

This script processes sequencing data from ChIP-seq and chromatin accessibility assays from fastq to bigwig.

OPTIONS:
   -d PROJECT_DIR	Path to your project data folder.
      	  		The script will create links to the centrally stored data in that location.

   -t DATA_TABLE 	Path to your data table in comma-separated csv format
   	  	  	Download template here: https://bit.ly/38VOMrc
			Ideally store this file in the location you specified as -d DATA_DIR

   -n NTHREADS   	Number of parallel threads
 
   -o OVERWRITE		Flag for overwriting existing files
EOF
}

# Print message if a file already exists
file_exists() {
	file=$(echo $1 | rev | cut -d'/' -f1,2 | rev) # trimmed path with only the last folder to avoid overcrowded print
    echo "$file exists and will not be overwritten. (Set the -o option for overwriting existing files)"
}

# ---------------------------------------------------

# parse arguments (more info at https://ndench.github.io/bash/parsing-bash-flags)
nthreads=1
overwrite=False
while getopts “:d:t:n:o” OPTION; do
	case $OPTION in
		d) project_dir=$(readlink -f $OPTARG) ;; # full path
		t) data_table=$OPTARG ;;
		n) nthreads=$OPTARG ;;
		o) overwrite=True ;;
		?) usage; exit 1 ;;
 	esac
done

# throw error if not all mandatory arguments are passed
if [ -z ${project_dir+x} ] || [ -z ${data_table+x} ] || [ -z ${nthreads+x} ] || [ -z ${overwrite+x} ]; then
	usage
	exit 1
fi

# create folder structures
mkdir -p ${project_dir}/bam ${project_dir}/bigwig

# parse data table and create fastq links
declare -a data # array indexed by integers
declare -A fastqs bams bigwigs experiments sequencing_types builds star_index_dirs # associative array indexed by strings
{
	echo "Fetching data"
	IFS="," read -r header || [ -n "$header" ] # read the header
	while IFS="," read -r row_raw || [ -n "$row_raw" ]; do # loop through rows
		# parse row from table and assign its values to variables
		row=$row_raw
		row=${row//$'\r'/} # remove carriage returns
		row=${row//$'\n'/} # and newlines from row string
		row=$(sed -e 's/,,/,NA,/g' -e 's/,,/,NA,/g' -e 's/,,/,NA,/g' -e 's/,$/,NA/g' <<< "$row") # replace missing values (two commas) with "NA". (Repeated first rule 3 times to take care of neighboring columns with missing data, e.g. ...,value,,,value,...)
		arr=(${row//,/ }) # convert row string to array
		if [[ -d ${arr[0]} ]]; then # in-house data, valid path to directory
			data_source="mpimg"
			seqcore_folder=${arr[0]}
			library_number=${arr[9]}
			id=$library_number # id for central data table
			flow_cell=${arr[10]}
		elif [[ ${arr[0]} == SRR* ]]; then # GEO data
			data_source="GEO"
			SRR_number=${arr[0]}
			id=$SRR_number # id for central data table
		elif [[ ${arr[0]} == ENC* ]]; then # ENCODE data
			data_source="ENCODE"
			ENC_accession=${arr[0]}
			id=$ENC_accession # id for central data table
		else
			echo "Error: Value in first column must either be a valid path to the seqcore directory containing the fastqs, a SRR number from GEO, or an ENCODE experiment accession number" && exit 1
		fi
		feature=${arr[1]}
		tissue=${arr[2]}
		stage=${arr[3]}
		build=${arr[4]}
		species=$(sed -e 's/[0-9]//g' <<< "$build")
		condition=${arr[5]}
		biological_replicate=${arr[6]}
		if [[ $biological_replicate != Rep* ]]; then echo "Error: Values in column 'biological_replicate' must start with 'Rep', e.g. 'Rep1'." && exit 1; fi
		sample=${feature}_${tissue}_${stage}_${species}_${build}_${condition}_${biological_replicate}
		sample=$(sed -e 's/_NA_/_/g' <<< "$sample") # remove occurrences of 'NA' from sample name (in case a particular column was left empty, e.g. tissue for ESC)
		fastq_sample=${feature}_${tissue}_${stage}_${species}_${condition}_${biological_replicate} # the fastq_sample contains the species name instead of the build name
		fastq_sample=$(sed -e 's/_NA_/_/g' <<< "$fastq_sample")
		bam_sample=${feature}_${tissue}_${stage}_${build}_${condition}_${biological_replicate} # the bam_sample contains the build name instead of the species name
		bam_sample=$(sed -e 's/_NA_/_/g' <<< "$bam_sample")
		data_dir=/project/MDL_ChIPseq/data/epigenome/${species} # set location for central data storage
		mkdir -p $data_dir/fastq $data_dir/bam $data_dir/bigwig # create subfolders
		bam=${data_dir}/bam/${bam_sample}.bam
 		species_dict[$build]=$species
		builds[$bam_sample]=$build
 		sequencing_type=${arr[7]}
		sequencing_types[$bam_sample]=$sequencing_type
		experiments[$bam_sample]=${arr[8]}
		if [[ ${experiments[$bam_sample]} != "ChIP-seq" ]] && [[ ${experiments[$bam_sample]} != "chromatin-accessibility" ]]; then echo "Error: Values in column 'experiment' must be from the following list: ['ChIP-seq', 'chromatin-accessibility']" && exit 1; fi
		if [[ $sequencing_type == 'paired-end' ]]; then reads=( "R1" "R2" ); elif [[ $sequencing_type == 'single-end' ]]; then reads=( "R1" ); else echo "sequencing_type must be either 'single-end' or 'paired-end'" && exit 1; fi

		# manage central data table: add row entry to $data or link existing data to user project folder and continue loop
		data_row="${bam_sample}\t${id}\t${USER}" # row in data.tsv without the date
		[[ $(grep -F "$data_row" ${data_dir}/data.tsv) != "" ]] && echo hallo
		if [[ -f ${data_dir}/data.tsv ]]; then
			if [[ $overwrite == False ]] && [[ $(grep -F "$data_row" ${data_dir}/data.tsv) != "" ]]; then # data entry does already exist, create links and continue loop
				echo -e "\n${data_row}: Data exists and will be linked to your project folder\nSet the -o option for overwriting existing files"
				[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam ]] && ln -s ${data_dir}/bam/${bam_sample}.rmdup.bam ${project_dir}/bam/
				[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam.csi ]] && ln -s ${data_dir}/bam/${bam_sample}.rmdup.bam.csi ${project_dir}/bam/
				[[ ! -e ${project_dir}/bigwig/${bam_sample}.rpkm.bw ]] && ln -s ${data_dir}/bigwig/${bam_sample}.rpkm.bw ${project_dir}/bigwig/
				continue
			else # data entry does not yet exist or must be overwritten
				[[ ! "${data[@]}" =~ "${data_row}" ]] && data+=($data_row) # only if $data_row is not yet in $data (which can occur with multiple sequencing rows)
			fi
		else # data table does not yet exist at all
			[[ ! "${data[@]}" =~ "${data_row}" ]] && data+=($data_row)
		fi

		# create symbolic links for original fastq files
		if [[ $data_source == "mpimg" ]]; then
			for read in "${reads[@]}"; do
			    file=$seqcore_folder/*${library_number}*${flow_cell}_${read}*fastq.gz
				link=${data_dir}/fastq/${fastq_sample}_${flow_cell}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e $link ]]; then
					ln -sf $file $link
				else
					:
					# file_exists $link
				fi
				fastq=${data_dir}/fastq/${fastq_sample}_${read}.fastq.gz
				fastqs[$fastq]+=$link, # store the current link in an associative array with the name of the merged fastq as the key (in case of mutliple sequencing runs). Adding a comma allows to split them later.
				[[ ${bams[$bam]} != *"$fastq"* ]] && bams[$bam]+=$fastq, # add the 'merged' file name (without flow-cell) only if it is not already there (in case of multiple sequencing runs)
			done
		elif [[ $data_source == "GEO" ]]; then
			echo "Downloading $SRR_number from GEO"
			if [[ $overwrite == True ]] || ! ls ${data_dir}/fastq/${SRR_number}_?.fastq.gz 1> /dev/null 2>&1; then # the second condition checks if any (R1 or R2) fastq.gz file exists for this SRR
				fastq-dump --gzip --split-files -O ${data_dir}/fastq $SRR_number # --split-files will produce separate files for R1 and R2 and add the suffices .1 and .2, respectively.
			fi
			for read in "${reads[@]}"; do # add sample description to SRR file names and rename _1 and _2 to R1 and R2
				i=$(echo $read | tr -d -c 0-9)
				fastq=${data_dir}/fastq/${fastq_sample}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e $fastq ]]; then
					ln -sf ${data_dir}/fastq/${SRR_number}_${i}.fastq.gz $fastq
				else
					:
					# file_exists $fastq
				fi
				fastqs[$fastq]+=$fastq, # here, key and value are identical except for the comma in the value, because GEO data is 1 run per experiment without indication of flow-cell number
				bams[$bam]+=$fastq,
			done
		elif [[ $data_source == "ENCODE" ]]; then
			for read in "${reads[@]}"; do
				ENC_url=$(prun mdl fetch_encode_url.py ${ENC_accession} $(sed -e 's/[^0-9]//g' <<< "${biological_replicate}") $(sed -e 's/[^0-9]//g' <<< "${read}") $sequencing_type)
				ENC_file=${data_dir}/fastq/${ENC_url##*/}
				echo "Downloading ${ENC_url} from ENCODE"
				fastq=${data_dir}/fastq/${fastq_sample}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e  $ENC_file ]]; then # download
					curl -o $ENC_file -O -L $ENC_url
				else
					:
					# file_exists $ENC_file
				fi
				if [[ $overwrite == True ]] || [[ ! -e $fastq ]]; then # link
					ln -sf $ENC_file $fastq
				else
					:
					# file_exists $fastq
				fi				
			  	fastqs[$fastq]+=$fastq, # here, key and value are identical except for the comma in the value, because ENCODE data is 1 run per experiment without indication of flow-cell number
				bams[$bam]+=$fastq,
			done
		fi
	done
} < $data_table

# merge fastq's of potential multiple sequencing runs
merge_print_flag=0 # merge_print_flag=0 # such that the message about merging the fastq files is only printed once, and only if any fastqs are merged
for bam in "${!bams[@]}"; do
	bam_sample=$(sed -e 's/\.bam//g' <<< "$bam" | rev | cut -f1 -d/ | rev)
	build=${builds[$bam_sample]}
	species=${species_dict[$build]}
	if [[ $sequencing_types[$bam_sample] == 'paired-end' ]]; then reads=( "R1" "R2" ); elif [[ $sequencing_types[$bam_sample] == 'single-end' ]]; then reads=( "R1" ); fi
	for read in ${reads[@]}; do
		fastq=$(sed -e "s/${build}/${species}/g" -e "s/\.bam/\_${read}\.fastq.gz/g" -e 's/\/bam\//\/fastq\//g' <<< "$bam") # merged fastq
		if [[ $overwrite == True ]] || [[ ! -e $fastq ]]; then
			n=$(awk -F, '{print NF-1}' <<< "${fastqs[$fastq]}") # count the number of commas (i.e. the number of sequencing runs)
			if [[ $n == 1 ]]; then # one run: rename, remove flow-cell number
				single_fastq=$(sed 's/,//g' <<< "${fastqs[$fastq]}")
				[[ $single_fastq != $fastq ]] && mv $single_fastq $fastq
			elif [[ $n == 2 ]]; then # multiple runs: merge and remove flow-cell numbers
				[[ $merge_print_flag == 0 ]] && echo "Merging fastq files from multiple sequencing runs" && merge_print_flag=1
				cat ${fastqs[$fastq]//$','/$' '} > $fastq
			fi
		else
			file_exists $fastq
		fi
	done
done

# Align data and produce bigwig tracks
# for bam in "${!bams[@]}"; do
for bam in "${!bams[@]}"; do
	bam_sample=$(sed -e 's/\.bam//g' <<< "$bam" | rev | cut -f1 -d/ | rev) # here, sample contains the build instead of the species
	bam_rmdup=${data_dir}/bam/${bam_sample}.rmdup.bam
	build=${builds[$bam_sample]}
	echo -e "\n${bam_sample}:"
	if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup ]]; then
		# align fastq to bam
		echo "Aligning reads"
		if [[ $overwrite == True ]] || [[ ! -e $bam ]]; then
			# check if STAR genome index exists
			star_index_dir=/project/MDL_ChIPseq/data/genome/star_index/$build
			star_index_dirs[$build]=$star_index_dir # for the unloading at the end of the script
			if [[ ! -e $star_index_dir ]]; then # if it still does not exist, exit and prompt the user to create index first
				echo "STAR genome not found. Create it from a fasta file by running 'prun mdl star_index [genome.fa]'"
				exit 1
			fi
			# align
			current_file=$bam
			prun mdl star_align -b $build -i $star_index_dir -n $nthreads -o $bam ${bams[$bam]//','/' '} # the 'bams' array holds the associated fastqs (R1 / R2 for paired-end) at the entry with the bam name as key
			current_file=NA
		else
			file_exists $bam
		fi

		# fill in mate coordinates for paired-end. this requires the bam to be sorted by name. later, markdup again requires the bam to be sorted by coordinate, which we need two steps of sorting here.
		if [[ ${sequencing_types[$bam_sample]} == "paired-end" ]]; then
			# sort bam by name
			echo "Sorting alignment by name"
			bam_nsort=${data_dir}/bam/${bam_sample}.nsort.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_nsort ]]; then
				current_file=$bam_nsort
				samtools sort -n --threads $nthreads -o $bam_nsort $bam # -n flag for sorting by name
				current_file=NA
			else
				file_exists $bam_nsort
			fi

			# fixmate
			echo "Filling in mate coordinates"
			bam_fixmate=${data_dir}/bam/${bam_sample}.fixmate.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_fixmate ]]; then
				current_file=$bam_fixmate
				samtools fixmate -m --threads $nthreads $bam_nsort $bam_fixmate
				current_file=NA
			else
				file_exists $bam_fixmate
			fi

			# sort bam by coordinate
			echo "Sorting alignment by coordinate"
			bam_csort=${data_dir}/bam/${bam_sample}.csort.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_csort ]]; then  
				current_file=$bam_csort
				samtools sort --threads $nthreads -o $bam_csort $bam_fixmate
				current_file=NA
			else
				file_exists $bam_csort
			fi
			bam=$bam_csort
		fi
		
		# remove duplicates
		echo "Removing duplicates"
		if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup ]]; then
			current_file=$bam_rmdup
			samtools markdup -s -r --threads $nthreads $bam $bam_rmdup 2> ${data_dir}/bam/log/${bam_sample}.Log.rmdup.out
			current_file=NA
		else
			file_exists $bam_rmdup
		fi
	else
		file_exists $bam_rmdup
	fi

	# index bam
	echo "Indexing bam-file"
	if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup.csi ]]; then
		current_file=$bam_rmdup.csi
		samtools index -c $bam_rmdup # index bam-file: use -c for .csi format instead of .bai, allowing for chromosome sizes larger than 500 Mb (e.g. carPer2.1)
		current_file=NA
	else
		file_exists ${bam}.csi
	fi

	# produce bigwig track
	echo "Producing bigwig track"
	bigwig=${data_dir}/bigwig/${bam_sample}.rpkm.bw
 	if [[ ${experiments[$bam_sample]} == "ChIP-seq" ]]; then center_reads_flag="--centerReads"; else center_reads_flag=""; fi # center reads for ChIP-seq experiments, not for chromatin-accessiblity
	if [[ $overwrite == True ]] || [[ ! -e $bigwig ]]; then
		current_file=$bigwig
		bamCoverage --binSize 10 --normalizeUsing RPKM $center_reads_flag --minMappingQuality 30 -p $nthreads -b $bam_rmdup -o $bigwig
		current_file=NA
	else
		file_exists $bigwig
	fi

	# create links to user project folder
	echo "Linking the data to your project folder"
	[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam ]] && ln -s ${data_dir}/bam/${bam_sample}.rmdup.bam ${project_dir}/bam/
	[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam.csi ]] && ln -s ${data_dir}/bam/${bam_sample}.rmdup.bam.csi ${project_dir}/bam/
	[[ ! -e ${project_dir}/bigwig/${bam_sample}.rpkm.bw ]] && ln -s ${data_dir}/bigwig/${bam_sample}.rpkm.bw ${project_dir}/bigwig/
done

# manage central data table
for data_row in ${data[@]}; do
	if [[ -f ${data_dir}/data.tsv ]]; then
		if [[ $overwrite == True ]] && [[ $(grep -F "$data_row" ${data_dir}/data.tsv) != "" ]]; then # row exists, overwrite with new date
			original=$(sed -e 's/\//\\\//g' <<< "${data_row}") # put row in regex format for sed's find and replace
			replacement=$(sed -e "s/$original/$original\\t$(date +%F)/g" <<< "$original") # replace the date
			sed -i "s/${original}./${replacement}" "${data_dir}/data.tsv"
		elif [[ $(grep -F "$data_row" ${data_dir}/data.tsv) == "" ]]; then # row does not exist yet, add
			echo -e "${data_row}\t$(date +%F)" >> ${data_dir}/data.tsv
		fi
	else # file does not exist yet, initialize
		echo -e "sample\tsource\tuser\tdate" > ${data_dir}/data.tsv
		echo -e "${data_row}\t$(date +%F)" >> ${data_dir}/data.tsv
	fi
	(head -n 1 ${data_dir}/data.tsv && tail -n +2 ${data_dir}/data.tsv | sort -k 1,1) > ${data_dir}data.tsv.sorted # sort table by feature (keep header first)
	mv ${data_dir}data.tsv.sorted ${data_dir}/data.tsv 
done

### CLEANUP
echo -e "\nCleaning up directory: Delete intermediate files"
# find ${data_dir}/fastq/ -type f ! -name 'SRR*fastq.gz' ! -name 'ENC*fastq.gz' -delete # remove merged fastq's, only keep links to seqcore files and downloaded SRR's
find ${data_dir}/fastq/ -type f -delete # remove all real fastq files (merged fastq's and downloaded), only keep links to seqcore files
find ${data_dir}/fastq/ -xtype l -delete # remove all symlinks that point to non-existing files (e.g. the previously removed SRR / ENC fastq's)
find ${data_dir}/bam/ -type f -name *.bam ! -name '*.rmdup.bam' -delete # remove any intermediate bam files, only keep final rmduped bam

echo -e "\nRead mapping statistics are located in ${data_dir}/bam/*.Log.final.out"
echo "Read duplicate statistics are located in ${data_dir}/bam/*.Log.rmdup.out"

echo -e "\nDone"

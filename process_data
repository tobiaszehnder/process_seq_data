#! /bin/bash

### Script to process ChIP-seq data from fastq to bigwig

set -eE # exit when any command fails (e), and allow error trapping (E)
shopt -s extglob # extended globbing. e.g. for expanding wildcards in variables

# ---------------------------------------------------
# Functions
# ---------------------------------------------------

# cleanup trap
trap 'cleanup ${current_file}' ERR INT # if the script fails or is aborted by the user (CTRL+C), remove the file that is currently being written.
cleanup() { if [[ $1 != "NA" ]]; then echo -e "\nRemoving $1"; rm -f $1; exit 0; fi }

# script usage
usage() {
cat << EOF
Usage: prun mdl ${0##*/} -d DATA_DIR -t DATA_TABLE -n NTHREADS [optional: -o]

This script processes sequencing data from ChIP-seq and chromatin accessibility assays from fastq to bigwig.

OPTIONS:
   -d PROJECT_DIR	Path to your project data folder.
      	  		The script will create links to the centrally stored data in that location.

   -t DATA_TABLE 	Path to your data table in comma-separated csv format
   	  	  	Download template here: https://bit.ly/38VOMrc
			Ideally store this file in the location you specified as -d DATA_DIR

   -n NTHREADS   	Number of parallel threads
 
   -o OVERWRITE		Flag for overwriting existing files

   -s STAR_PARAMS	Optional: any number of STAR parameters for the alignment in double quotes.
   	  				Example: -s "--outFilterMultimapNmax 1 --outSAMmultNmax 1"
EOF
}

# Print message if a file already exists
file_exists() {
	# file=$(echo $1 | rev | cut -d'/' -f1,2 | rev) # trimmed path with only the last folder to avoid overcrowded print
	file=$1
	echo "The following file exists and will not be overwritten: $file"
}

# ---------------------------------------------------

# parse arguments (more info at https://ndench.github.io/bash/parsing-bash-flags)
nthreads=1
overwrite=False
local=False # Flag for saving files locally on the user's project folder instead of centrally. This option will not be advertised.
star_params=" "
while getopts “:d:t:n:s:ol” OPTION; do
	case $OPTION in
		d) project_dir=$(realpath $OPTARG) ;; # full path
		t) data_table=$(realpath $OPTARG) ;;
		n) nthreads=$OPTARG ;;
		o) overwrite=True ;;
		l) local=True ;;
		s) star_params=$OPTARG ;;
		?) usage; exit 1 ;;
 	esac
done

# throw error if not all mandatory arguments are passed
if [ -z ${project_dir+x} ] || [ -z ${data_table+x} ]; then
	usage
	exit 1
fi

# create folder structures
mkdir -p ${project_dir}/bam ${project_dir}/bigwig
error_log=${data_table}.errorlog
echo -n "" > ${error_log} # initialize empty error_log
current_file=NA # file to be deleted when job aborts

# parse data table and create fastq links
declare -a skip # array indexed by integers
declare -A fastqs bams bigwigs experiments sequencing_types builds star_index_dirs species_dict data srr_dict # associative array indexed by strings
{
	echo -e "Reading the data table"
	IFS="," read -r header || [ -n "$header" ] # read the header
	while IFS="," read -r row_raw || [ -n "$row_raw" ]; do # loop through rows
		# parse row from table and assign its values to variables
		row=$row_raw
		row=${row//$'\r'/} # remove carriage returns
		row=${row//$'\n'/} # and newlines from row string
		row=$(sed -e 's/,,/,NA,/g' -e 's/,,/,NA,/g' -e 's/,,/,NA,/g' -e 's/,$/,NA/g' <<< "$row") # replace missing values (two commas) with "NA". (Repeated first rule 3 times to take care of neighboring columns with missing data, e.g. ...,value,,,value,...)
		arr=(${row//,/ }) # convert row string to array
		feature=${arr[1]}
		tissue=${arr[2]}
		stage=${arr[3]}
		build=${arr[4]}
		species=$(sed -e 's/[0-9]//g' <<< "$build")
		condition=${arr[5]}
		biological_replicate="${arr[6]^}"
		if [[ $biological_replicate != Rep* ]]; then echo "Error: Values in column 'biological_replicate' must start with 'Rep', e.g. 'Rep1'." && exit 1; fi
		sample=${feature}_${tissue}_${stage}_${species}_${build}_${condition}_${biological_replicate}
		sample=$(sed -e 's/_NA_/_/g' <<< "$sample") # remove occurrences of 'NA' from sample name (in case a particular column was left empty, e.g. tissue for ESC)
		fastq_sample=${feature}_${tissue}_${stage}_${species}_${condition}_${biological_replicate} # the fastq_sample contains the species name instead of the build name
		fastq_sample=$(sed -e 's/_NA_/_/g' <<< "$fastq_sample")
		bam_sample=${feature}_${tissue}_${stage}_${build}_${condition}_${biological_replicate} # the bam_sample contains the build name instead of the species name
		bam_sample=$(sed -e 's/_NA_/_/g' <<< "$bam_sample")
		if [[ -d ${arr[0]} ]]; then # in-house data, valid path to directory
			data_source="mpimg"
			seqcore_folder=${arr[0]}
			library_number=${arr[9]}
			id=$library_number # id for central data table
			flow_cell=${arr[10]}
			fastq_sample="${fastq_sample}_${id}"
			bam_sample="${bam_sample}_${id}"
		elif [[ ${arr[0]} == SRR* ]]; then # GEO data
			data_source="GEO"
			SRR_number=${arr[0]}
			id=$SRR_number # id for central data table
			# Multiple sequencing runs have different SRR (run) numbers, but a common SRX (experiment) number. Save that internally for association during mergin.
			# Output files will be named with all SRRs, e.g. sample_SRR1_SRR2.bam
			srx=$(curl -s "https://www.ebi.ac.uk/ena/portal/api/filereport?accession=${SRR_number}&result=read_run&fields=study_accession,sample_accession,experiment_accession,run_accession,tax_id,scientific_name,fastq_ftp,submitted_ftp,sra_ftp&format=tsv&download=true" | tail -1 | cut -f3)
			echo $srx
			echo $SRR_number
			srr_dict[$srx]+=$SRR_number,
			fastq_sample_srr="${fastq_sample}_${SRR_number}"
			fastq_sample_srx="${fastq_sample}_${srx}"
			bam_sample="${bam_sample}_${srx}"
		elif [[ ${arr[0]} == ENC* ]]; then # ENCODE data
			data_source="ENCODE"
			ENC_accession=${arr[0]}
			id=$ENC_accession # id for central data table
			fastq_sample="${fastq_sample}_${id}"
			bam_sample="${bam_sample}_${id}"
		else
			echo "Error: Value in first column must either be a valid path to the seqcore directory containing the fastqs, a SRR number from GEO, or an ENCODE experiment accession number" && exit 1
		fi 
		if [[ $local == False ]]; then
			data_dir=/project/MDL_ChIPseq/data/epigenome/${species} # set location for central data storage
		elif [[ $local == True ]]; then
			data_dir=$project_dir
		fi
		mkdir -p $data_dir/fastq $data_dir/bam $data_dir/bigwig # create subfolders

		# skip if bam and bigwig exist
		if [[ -e ${data_dir}/bam/${bam_sample}.rmdup.bam ]] && [[ -e ${data_dir}/bigwig/${bam_sample}.rpkm.bw ]]; then
			if [[ $overwrite == False ]]; then
				continue
			fi
		fi

		# ##### for renaming (adding ${id} to the file names). not used anymore.
		# fastq_sample_old=${feature}_${tissue}_${stage}_${species}_${condition}_${biological_replicate} # the fastq_sample contains the species name instead of the build name
		# fastq_sample_old=$(sed -e 's/_NA_/_/g' <<< "$fastq_sample_old")
		# bam_sample_old=${feature}_${tissue}_${stage}_${build}_${condition}_${biological_replicate} # the bam_sample contains the build name instead of the species name
		# bam_sample_old=$(sed -e 's/_NA_/_/g' <<< "$bam_sample_old")
		# echo $bam_sample
		# rename $fastq_sample_old $fastq_sample ${data_dir}/fastq/${fastq_sample_old}_R?.fastq.gz || echo -n ""
		# rename $bam_sample_old $bam_sample ${data_dir}/bam/${bam_sample_old}.rmdup.bam || echo -n ""
		# rename $bam_sample_old $bam_sample ${data_dir}/bigwig/${bam_sample_old}.rpkm.bw || echo -n ""
		# #####
		
		bam=${data_dir}/bam/${bam_sample}.bam
 		species_dict[$build]=$species
		builds[$bam_sample]=$build
 		sequencing_type=${arr[7]}
		sequencing_types[$bam_sample]=$sequencing_type
		experiments[$bam_sample]=${arr[8]}
		if [[ ${experiments[$bam_sample]} != "ChIP-seq" ]] && [[ ${experiments[$bam_sample]} != "chromatin-accessibility" ]]; then echo "Error: Values in column 'experiment' must be from the following list: ['ChIP-seq', 'chromatin-accessibility']" && exit 1; fi
		if [[ $sequencing_type == 'paired-end' ]]; then reads=( "R1" "R2" ); elif [[ $sequencing_type == 'single-end' ]]; then reads=( "R1" ); else echo "sequencing_type must be either 'single-end' or 'paired-end'" && exit 1; fi

		# manage central data table: add row entry to $data or link existing data to user project folder and continue loop
		data_row="${bam_sample}" # row in data.tsv without the date
		if [[ -f ${data_dir}/data.tsv ]]; then
			if [[ $overwrite == False ]] && [[ $(grep ${bam_sample}$'\t'${id} ${data_dir}/data.tsv) != "" ]]; then # data entry does already exist, create links and continue loop
				echo -e "\n${data_row}: Data exists and will be linked to your project folder\nSet the -o option for overwriting existing files"
				[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam ]] && ln -sf ${data_dir}/bam/${bam_sample}.rmdup.bam ${project_dir}/bam/
				[[ ! -e ${project_dir}/bam/${bam_sample}.rmdup.bam.csi ]] && ln -sf ${data_dir}/bam/${bam_sample}.rmdup.bam.csi ${project_dir}/bam/
				[[ ! -e ${project_dir}/bigwig/${bam_sample}.rpkm.bw ]] && ln -sf ${data_dir}/bigwig/${bam_sample}.rpkm.bw ${project_dir}/bigwig/
				skip+=($bam_sample) # array containing all bam_samples that are already processed. use later to skip those bam_samples.
				continue
			else # data entry does not yet exist or must be overwritten
				[[ ! "${!data[@]}" =~ "$bam_sample" ]] && data[$bam_sample]=$data_row # only if $data_row is not yet in $data (which can occur with multiple sequencing rows)
			fi
		else # data table does not yet exist at all
			[[ ! "${!data[@]}" =~ "$bam_sample" ]] && data[$bam_sample]=$data_row
		fi
		
		# create symbolic links for original fastq files
		if [[ $data_source == "mpimg" ]]; then
			for read in "${reads[@]}"; do
			    file=$seqcore_folder/*${library_number}*${flow_cell}_${read}*fastq.gz
				link=${data_dir}/fastq/${fastq_sample}_${flow_cell}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e $link ]]; then
					ln -sf $file $link
				else
					:
					# file_exists $link
				fi
				fastq=${data_dir}/fastq/${fastq_sample}_${read}.fastq.gz
				fastqs[$fastq]+=$link, # store the current link in an associative array with the name of the merged fastq as the key (in case of mutliple sequencing runs). Adding a comma allows to split them later.
				[[ ${bams[$bam]} != *"$fastq"* ]] && bams[$bam]+=$fastq, # add the 'merged' file name (without flow-cell) only if it is not already there (in case of multiple sequencing runs)
			done
		elif [[ $data_source == "GEO" ]]; then # GEO DATA
			if [[ $overwrite == True ]] || ! ls ${data_dir}/fastq/${SRR_number}*.fastq.gz 1> /dev/null 2>&1; then # the second condition checks if any fastq.gz file exists for this SRR
				# echo -e "\nDownloading $SRR_number from GEO"
				[[ ! -e ${data_dir}/fastq/${SRR_number}.sra ]] && prefetch -p -f yes -o ${data_dir}/fastq/${SRR_number}.sra ${SRR_number} # download sra
				[[ ! -e ${data_dir}/fastq/${SRR_number}*fastq ]] && fasterq-dump --split-files -f -e ${nthreads} -p -o ${data_dir}/fastq/${SRR_number}.fastq ${data_dir}/fastq/${SRR_number}.sra # convert sra to fastq
				[[ -e ${data_dir}/fastq/${SRR_number}.fastq ]] && [[ -e ${data_dir}/fastq/${SRR_number}.sra ]] && rm ${data_dir}/fastq/${SRR_number}.sra # remove sra if fastq was created successfully
				for fn in ${data_dir}/fastq/${SRR_number}*fastq; do gzip -1 -v $fn; done # compress fastq (-1: fastest) # not really necessary as we anyways delete the fastq at the end, but easier for consistency
			fi
			nfiles=$(ls -1q ${data_dir}/fastq/${SRR_number}*fastq.gz | wc -l)
			if [[ ( $sequencing_type == 'single-end' && $nfiles == 2 ) || ( $sequencing_type == 'paired-end' && $nfiles == 1 ) ]]; then
				err="${bam_sample} is not ${sequencing_type}. Change in data table and resubmit job."
				echo -e "$err\nWritten error to ${error_log}"
				echo $err >> ${error_log}
				skip+=($bam_sample)
				continue
			fi
			for read in "${reads[@]}"; do # add sample description to SRR file names and rename _1 and _2 to R1 and R2
				i=$(echo $read | tr -d -c 0-9)
				fastq_srr=${data_dir}/fastq/${fastq_sample_srr}_${read}.fastq.gz
				fastq_srx=${data_dir}/fastq/${fastq_sample_srx}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e $fastq_srr ]]; then
					if [[ $sequencing_type == 'single-end' ]]; then
						mv ${data_dir}/fastq/${SRR_number}.fastq.gz $fastq_srr # single-end reads from GEO have no _1 at the end
					elif [[ $sequencing_type == 'paired-end' ]]; then
						mv ${data_dir}/fastq/${SRR_number}_${i}.fastq.gz $fastq_srr # paired-end reads from GEO have _1 / _2 at the end
					fi
				else
					: # file_exists $fastq
				fi
				fastqs[$fastq_srx]+=$fastq_srr, # key: merged fastq name (SRX), values: separate fastq names (SRR)
				bam_srx=$(sed -e "s/${SRR_number}/${srx}/g" <<< "$bam") # replace SRR_number with srx in bam file name, later substitute it back to all SRR numbers
				[[ ! "${bams[$bam_srx]}" =~ "$srx" ]] && bams[$bam_srx]=$fastq_srx # key: merged bam name (SRX), values: merged fastq names (SRX) (multiple if paired end). only add SRX if it is not already added.
			done
		elif [[ $data_source == "ENCODE" ]]; then
			for read in "${reads[@]}"; do
				fastq=${data_dir}/fastq/${fastq_sample}_${read}.fastq.gz
				if [[ $overwrite == True ]] || [[ ! -e $fastq ]]; then
					ENC_url=$(prun mdl fetch_encode_url.py ${ENC_accession} $(sed -e 's/[^0-9]//g' <<< "${biological_replicate}") $(sed -e 's/[^0-9]//g' <<< "${read}") $sequencing_type)
					if [[ $ENC_url == None ]];  then
						err="${bam_sample}: Download link for $ENC_accession not found. Download manually and restart script."
						echo -e "$err\nWritten error to ${error_log}"
						echo $err >> ${error_log}
						skip+=($bam_sample)
						continue
					fi
					# rename old ENC*.fastq.gz to new naming
					if [[ ! -e $fastq ]] && [[ -e ${data_dir}/fastq/$(echo $ENC_url | rev | cut -f1 -d "/" | rev) ]]; then
						mv ${data_dir}/fastq/$(echo $ENC_url | rev | cut -f1 -d "/" | rev) $fastq
					elif [[ $overwrite == True ]] || [[ ! -e  $fastq ]]; then # download
						echo -e "\nDownloading experiment ${ENC_accession} from ENCODE: ${ENC_url}"
						curl -o $fastq -O -L $ENC_url
					fi
				fi				
			  	fastqs[$fastq]+=$fastq, # here, key and value are identical except for the comma in the value, because ENCODE data is 1 run per experiment without indication of flow-cell number
				bams[$bam]+=$fastq,
			done
		fi
	done
} < $data_table

# merge fastq's of potential multiple sequencing runs
merge_print_flag=0 # merge_print_flag=0 # such that the message about merging the fastq files is only printed once, and only if any fastqs are merged
for bam in "${!bams[@]}"; do
	bam_sample=$(sed -e 's/\.bam//g' <<< "$bam" | rev | cut -f1 -d/ | rev)
	[[ "${skip[@]}" =~ "$bam_sample" ]] && continue # skip is an array containing all bam_samples that are already processed and have been linked to the user project folder.
	build=${builds[$bam_sample]}
	species=${species_dict[$build]}
	if [[ $sequencing_types[$bam_sample] == 'paired-end' ]]; then reads=( "R1" "R2" ); elif [[ $sequencing_types[$bam_sample] == 'single-end' ]]; then reads=( "R1" ); fi
	for read in ${reads[@]}; do
		# merged_fastq=$(sed -e "s/_${build}_/_${species}_/g" -e "s/\.bam/\_${read}\.fastq.gz/g" -e 's/\/bam\//\/fastq\//g' <<< "$bam") # merged fastq
		merged_fastqs=($(sed -e 's/,/ /g' <<< "${bams[$bam]}")) # array of merged fastqs for this bam (multiple for paired-end, R1 and R2)
		for elm in ${merged_fastqs[@]}; do [[ "$elm" =~ "$read" ]] && merged_fastq=$elm; done # save the merged_fastq of the currently iterated read (R1 or R2)
		unmerged_fastqs="${fastqs[$merged_fastq]}"
		if [[ $overwrite == True ]] || [[ ! -e $merged_fastq ]]; then
			n=$(awk -F, '{print NF-1}' <<< ${unmerged_fastqs}) # count the number of commas-delimited fields (i.e. the number of sequencing runs)
			echo $n
			if [[ $n == 1 ]]; then # one run: rename, remove flow-cell number
				single_fastq=$(sed 's/,//g' <<< "${unmerged_fastqs}")
				[[ $single_fastq != $merged_fastq ]] && ln -sf $single_fastq $merged_fastq
			elif [[ $n > 2 ]]; then # multiple runs: merge and remove flow-cell numbers
				[[ $merge_print_flag == 0 ]] && echo "Merging fastq files from multiple sequencing runs" && merge_print_flag=1
				cat $(sed -e "s/,/ /g" <<< "${unmerged_fastqs}") > $merged_fastq
			   	# cat ${unmerged_fastqs//$','/$' '} > $merged_fastq
			fi
		else
			:
			# file_exists $merged_fastq
		fi
	done
done

# Align data and produce bigwig tracks
# for bam in "${!bams[@]}"; do
for bam in "${!bams[@]}"; do
	bam_sample=$(sed -e 's/\.bam//g' <<< "$bam" | rev | cut -f1 -d/ | rev) # here, sample contains the build instead of the species
	[[ "${skip[@]}" =~ "$bam_sample" ]] && continue # skip is an array containing all bam_samples that are already processed and have been linked to the user project folder.
	build=${builds[$bam_sample]}
	species=${species_dict[$build]}
	if [[ $local == False ]]; then
		data_dir=/project/MDL_ChIPseq/data/epigenome/${species} # set location for central data storage
	elif [[ $local == True ]]; then
		data_dir=$project_dir
	fi
	echo -e "\n${bam_sample}:"
	bam_rmdup=${data_dir}/bam/${bam_sample}.rmdup.bam
	if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup ]]; then
		# align fastq to bam
		echo "Aligning reads"
		if [[ $overwrite == True ]] || [[ ! -e $bam ]]; then
			# check if STAR genome index exists
			star_index_dir=/project/MDL_ChIPseq/data/genome/star_index/$build
			star_index_dirs[$build]=$star_index_dir # for the unloading at the end of the script
			if [[ ! -e $star_index_dir ]]; then # if it still does not exist, exit and prompt the user to create index first
				echo "STAR genome not found. Create it from a fasta file by running 'prun mdl star_index [genome.fa]'"
				exit 1
			fi
			# align
			current_file=$bam
			prun mdl star_align -b $build -i $star_index_dir -n $nthreads -s "'$star_params'" -o $bam ${bams[$bam]//','/' '} # the 'bams' array holds the associated fastqs (R1 / R2 for paired-end) at the entry with the bam name as key
			current_file=NA
		else
			file_exists $bam
		fi

		# fill in mate coordinates for paired-end. this requires the bam to be sorted by name. later, markdup again requires the bam to be sorted by coordinate, which we need two steps of sorting here.
		if [[ ${sequencing_types[$bam_sample]} == "paired-end" ]]; then
			# sort bam by name
			echo "Sorting alignment by name"
			bam_nsort=${data_dir}/bam/${bam_sample}.nsort.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_nsort ]]; then
				current_file=$bam_nsort
				samtools sort -n --threads $nthreads -o $bam_nsort $bam # -n flag for sorting by name
				current_file=NA
			else
				file_exists $bam_nsort
			fi

			# fixmate
			echo "Filling in mate coordinates"
			bam_fixmate=${data_dir}/bam/${bam_sample}.fixmate.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_fixmate ]]; then
				current_file=$bam_fixmate
				samtools fixmate -m --threads $nthreads $bam_nsort $bam_fixmate
				current_file=NA
			else
				file_exists $bam_fixmate
			fi

			# sort bam by coordinate
			echo "Sorting alignment by coordinate"
			bam_csort=${data_dir}/bam/${bam_sample}.csort.bam
			if [[ $overwrite == True ]] || [[ ! -e $bam_csort ]]; then  
				current_file=$bam_csort
				samtools sort --threads $nthreads -o $bam_csort $bam_fixmate
				current_file=NA
			else
				file_exists $bam_csort
			fi
			bam=$bam_csort
		fi
		
		# remove duplicates
		echo "Removing duplicates"
		if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup ]]; then
			current_file=$bam_rmdup
			{ # try
				samtools markdup -s -r --threads $nthreads $bam $bam_rmdup 2> ${data_dir}/bam/log/${bam_sample}.Log.rmdup.out
			} || { # else
				# samtools markdup will fail if sequencing_type is wrong (e.g. SE instead of PE). Remove all bam of this sample, skip sample and save info in error_log
				err=$(grep "error" ${data_dir}/bam/log/${bam_sample}.Log.rmdup.out)
				if [[ $err != "" ]]; then
	 				err="$bam_sample: Duplicate removal failed. Removed all processed bam files for this sample. Check if sequencing_type in data table is correct."
					echo -e "$err\nWritten error to ${error_log}"
					echo -e "$(date): $err" >> ${error_log}
					rm ${data_dir}/bam/${bam_sample}*
					skip+=($bam_sample)
					continue
				fi
			}
			current_file=NA
		else
			file_exists $bam_rmdup
		fi
	else
		file_exists $bam_rmdup
	fi

	# index bam
	echo "Indexing bam-file"
	if [[ $overwrite == True ]] || [[ ! -e $bam_rmdup.csi ]]; then
		current_file=$bam_rmdup.csi
		samtools index -c $bam_rmdup # index bam-file: use -c for .csi format instead of .bai, allowing for chromosome sizes larger than 500 Mb (e.g. carPer2.1)
		current_file=NA
	else
		file_exists ${bam}.csi
	fi

	# produce bigwig track
	echo "Producing bigwig track"
	bigwig=${data_dir}/bigwig/${bam_sample}.rpkm.bw
 	if [[ ${experiments[$bam_sample]} == "ChIP-seq" ]]; then center_reads_flag="--centerReads"; else center_reads_flag=""; fi # center reads for ChIP-seq experiments, not for chromatin-accessiblity
	if [[ $overwrite == True ]] || [[ ! -e $bigwig ]]; then
		current_file=$bigwig
		mkdir -p ${data_dir}/bigwig/log
		bamCoverage --binSize 10 --normalizeUsing RPKM $center_reads_flag --minMappingQuality 30 -p $nthreads -b $bam_rmdup -o $bigwig > ${data_dir}/bigwig/log/${bam_sample}.bamCoverage.log
		current_file=NA
	else
		file_exists $bigwig
	fi

	# GEO: rename SRX with SRR1_SRR2 in case of multiple sequencing runs
	bam_srx=$(echo $bam_rmdup | rev | cut -f1 -d'_' | rev | cut -f1 -d'.')
	if [[ "${!srr_dict[@]}" =~ "$bam_srx" ]]; then
		bam_SRRs=($(sed -e 's/\(.*\),/\1/' -e 's/,/ /g' <<< "${srr_dict[$bam_srx]}")) # put SRRs into an array (remove last trailing comma, replace all other commas with spaces)
		bam_SRRs_string=$(echo "${bam_SRRs[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' ' | tr ' ' '_' | sed -e 's/\(.*\)_/\1/') # save unique SRRs in a '_'-delimited string
		new_bam_name=$(sed -e "s/${bam_srx}/${bam_SRRs_string}/g" <<< "$bam_rmdup") # replace old srx with new SRR1_SRR2 string
		mv $bam_rmdup $new_bam_name
		bam_sample=$(sed -e "s/${bam_srx}/${bam_SRRs_string}/g" <<< "$bam_sample")
	fi
	
	# create links to user project folder
	if [[ $local == False ]]; then
		echo "Linking the data to your project folder"
		ln -sf ${data_dir}/bam/${bam_sample}.rmdup.bam ${project_dir}/bam/
		ln -sf ${data_dir}/bam/${bam_sample}.rmdup.bam.csi ${project_dir}/bam/
		ln -sf ${data_dir}/bigwig/${bam_sample}.rpkm.bw ${project_dir}/bigwig/
	fi

	# manage central data table
	data_row="${data[$bam_sample]}\t${USER}"
	if [[ -f ${data_dir}/data.tsv ]]; then
		if [[ $overwrite == True ]] && [[ $(grep -F ${data[$bam_sample]}$'\t'${USER} ${data_dir}/data.tsv) != "" ]]; then # row exists, overwrite with new date
			original=$(sed -e 's/\//\\\//g' <<< "${data_row}") # put row in regex format for sed's find and replace
			replacement=$(sed -e "s/$original/$original\\t$(date +%F)/g" <<< "$original") # replace the date
			sed -i "s/${original}./${replacement}" "${data_dir}/data.tsv"
		elif [[ $(grep -F "${data_row}" ${data_dir}/data.tsv) == "" ]]; then # row does not exist yet, add
			echo -e "${data_row}\t$(date +%F)" >> ${data_dir}/data.tsv
		fi
	else # file does not exist yet, initialize
		echo -e "sample\tsource\tuser\tdate" > ${data_dir}/data.tsv
		echo -e "${data_row}\t$(date +%F)" >> ${data_dir}/data.tsv
	fi
	(head -n 1 ${data_dir}/data.tsv && tail -n +2 ${data_dir}/data.tsv | sort -k 1,1) > ${data_dir}/data.tsv.sorted # sort table by feature (keep header first)
	mv ${data_dir}/data.tsv.sorted ${data_dir}/data.tsv
	
	# clean up
	find ${data_dir}/fastq -type f -name ${fastq_sample}* -delete # remove all real fastq files (merged fastq's and downloaded), only keep links to seqcore files
	find ${data_dir}/fastq/ -xtype l -name ${fastq_sample}* -delete # remove all symlinks that point to non-existing files (e.g. the previously removed SRR / ENC fastq's)
	find ${data_dir}/bam/ -type f -name ${bam_sample}*.bam ! -name '*.rmdup.bam' -delete # remove any intermediate bam files, only keep final rmduped bam
done

echo -e "\nRead mapping statistics are located in ${data_dir}/bam/log/*.Log.final.out"
echo "Read duplicate statistics are located in ${data_dir}/bam/log/*.Log.rmdup.out"

echo -e "\nDone"
